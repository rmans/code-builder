---
id: TASK-2025-09-07-T6.3-eval-integration
title: Evaluation Integration â€“ evaluate-code
description: Use builder/evaluators/objective.py; router cb eval â†’ eval:objective; rule @rules/evaluate-code
status: completed
created: 2025-09-07
updated: 2025-09-07
owner: qa-agent
domain: evaluation
priority: 8
agent_type: backend
dependencies: [T6.1]
tags: [evaluation, commands]
---

# Task: Evaluation Integration â€“ evaluate-code

## Phases
### Phase 1: ðŸš€ Implementation
- [x] CLI flags: `--target <path|task|phase>`
- [x] Outputs: `cb_docs/eval/*.json|md`; fail â†’ non-zero exit

### Phase 2: ðŸ§ª Testing
- [x] Evaluate generated docs and task results

### Phase 3: ðŸ“š Documentation
- [x] Add to `evaluate.md`

### Phase 4: ðŸ§¹ Cleanup
- [x] Stable output schema

### Phase 5: ðŸ’¾ Commit
- [x] Commit + sync

## Acceptance Criteria
- [x] Evaluation works on any artifact; CI-friendly exit codes

## Completion Summary

**Status**: âœ… COMPLETED

**Implementation Details**:
- âœ… Created `eval:objective` CLI command with comprehensive evaluation capabilities
- âœ… Integrated with existing `builder/evaluators/objective.py` module
- âœ… Added support for multiple target types (path, task, phase)
- âœ… Implemented JSON and Markdown output formats
- âœ… Fixed configuration access issues in objective evaluator
- âœ… Added simple router mapping: `eval` â†’ `eval:objective`

**Generated Files**:
- âœ… `builder/core/cli/evaluation_commands.py` - CLI command implementation
- âœ… `.cb/commands/evaluate-code.md` - Command definition
- âœ… `.cursor/rules/evaluate-code.md` - Cursor rule integration
- âœ… `cb_docs/eval/` - Evaluation results directory

**Testing Results**:
- âœ… `eval:objective` command works with current directory
- âœ… `simple eval` correctly maps to `eval:objective`
- âœ… JSON output format generates proper evaluation results
- âœ… Markdown output format creates readable reports
- âœ… Exit codes properly reflect success/failure
- âœ… Evaluation metrics include tests, coverage, lint, spell, guardrails, and overall score

**Key Features**:
- **Objective Metrics**: Automated evaluation using multiple tools
- **Multiple Targets**: Evaluate paths, tasks, or phases
- **Flexible Output**: JSON or Markdown reports
- **CI-Friendly**: Proper exit codes for automation
- **Comprehensive Scoring**: Tests, coverage, linting, spelling, and guardrails
- **Simple Router Integration**: `cb simple eval` maps to `cb eval:objective`

**Integration Points**:
- Integrates with existing `builder/evaluators/objective.py` module
- Uses simple router for easy access via `cb simple eval`
- Generates results in `cb_docs/eval/` directory
- Provides both JSON and Markdown output formats
- Supports CI/CD integration with proper exit codes
